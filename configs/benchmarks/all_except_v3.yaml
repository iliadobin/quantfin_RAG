# Benchmark config: run all RAG pipelines except v3 (multiquery)

version: "1.0"

corpus_profile: "public"
index_strategy: "fixed"

datasets:
  - ds1
  - ds2
  - ds3
  - ds4
  - ds5

pipelines:
  - rag_v1_dense
  - rag_v2_hybrid
  - rag_v4_parent_child
  - rag_v5_evidence

pipeline_params:
  top_k: 20
  rerank_top_k: 10
  temperature: 0.0
  max_tokens: 1500

evaluation:
  # NOTE: scripts/run_benchmark.py currently creates BenchmarkRunner(llm_client=None),
  # so judge metrics will be skipped even if enabled. Keep it false for now.
  compute_llm_judge: false
  judge_batch_size: 5
  judge_sample_rate: 1.0

cache:
  enable_prediction_cache: true
  enable_embedding_cache: true

output:
  run_name: "all_except_v3"
  output_dir: "data/runs"
  save_predictions: true
  generate_reports: true

token_budget:
  max_total_tokens: 2000000
  warn_threshold: 1600000
  enable_monitoring: true

reproducibility:
  seed: 42
  deterministic: true


